---
layout: blog_post
title: Understanding Generative Adversarial Networks
date: 2025-08-21
tag: [generative modeling, adversarial training]
author: Chengpei Wu
excerpt: This post provides an introduction to Generative Adversarial Networks (GANs).
---

# Generative Adversarial Networks

Generative Adversarial Networks (GANs) are a class of machine learning frameworks designed for generative modeling by pitting two neural networks against each other in a game-theoretic setting. The two networks are the **generator** and the **discriminator**. The generator creates fake data samples, while the discriminator evaluates them against real data samples, providing feedback to the generator. This adversarial process drives both networks to improve, resulting in the generation of high-quality synthetic data.


## 1. Adversarial training

The training process of GANs involves alternating between training the discriminator and the generator. The discriminator is trained to maximize its ability to distinguish between real and fake samples, while the generator is trained to minimize the discriminator's ability to classify its samples as fake.

1. **Discriminator Training**: The discriminator is presented with a batch of real samples from the training dataset and a batch of fake samples generated by the generator. It learns to output high probabilities for real samples and low probabilities for fake samples.

2. **Generator Training**: The generator is then trained to produce samples that the discriminator classifies as real. This is done by feeding random noise into the generator and updating its weights based on the discriminator's feedback.

This adversarial process continues until the generator produces high-quality samples that are indistinguishable from real data, or until a predefined number of training iterations is reached.

The training objective is as follow:

$$
\min_{G} \max_{D} \left \{\mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_{z}(z)}[\log(1 - D(G(z)))]\right \}
$$

## 2. Understanding GAN (Take 1, Nash Equilibrium and Jensen-Shannon Divergence)

From the perspective of game theory, GANs can be viewed as a two-player zero-sum game where the generator and discriminator are competing against each other. The generator aims to produce samples that are indistinguishable from real data, while the discriminator aims to correctly classify samples as real or fake.

In this context, a Nash Equilibrium occurs when both players reach a state where neither can improve their strategy given the strategy of the other player. For GANs, this means that the generator produces high-quality samples that the discriminator cannot easily classify as fake, and the discriminator becomes highly skilled at identifying real versus fake samples.

Note that generative modeling is learning to approximate the true data distribution 
$$p(x)$$. **In the case of GANs, we will prove that the Nash Equilibrium occurs when the generator's distribution $$q(x)$$ matches the true data distribution $$p(x)$$.**

**Proof:**
to be added...